# Serify — Complete Build Specification
> Feed this entire document to the agent. Every screen, every feature, every interaction, every state.

---

## What You Are Building

Serify is a **context-aware learning reflection engine**. A user pastes a YouTube link, article URL, PDF, or raw notes. The system extracts key concepts from the content, generates intelligent free-text questions, analyzes the user's answers, and returns a deep diagnostic feedback report showing exactly what they understood, what was shallow, what was missing, and where their confidence didn't match their actual knowledge.

This is **not a quiz app**. It is a metacognition and active recall tool. The tone throughout is diagnostic and curious — never evaluative or punishing. Language like "Here's what you haven't connected yet" not "You got this wrong."

The existing prototype barely works. You are rebuilding it properly from scratch or heavily refactoring it. Every screen below must be fully functional, connected, and polished.

---

## Tech Stack

- **Frontend:** React + Tailwind CSS
- **Backend:** Node.js / Express (or Next.js API routes)
- **AI Layer:** Anthropic Claude API (`claude-sonnet-4-6` model) for concept extraction, question generation, answer analysis, and feedback generation
- **Database:** PostgreSQL (users, sessions, concepts, answers, feedback)
- **Auth:** Clerk or Supabase Auth (email + Google OAuth)
- **File handling:** PDF.js or pdf-parse for PDF extraction, youtube-transcript or similar for YouTube transcripts, cheerio/puppeteer for article scraping
- **Storage:** S3 or Supabase Storage for uploaded PDFs

---

## Design System

Apply this consistently across every screen.

**Colors**
```
--bg:            #F7F6F2   (warm off-white, page background)
--surface:       #FFFFFF   (cards, panels)
--border:        #E8E6DF   (all borders)
--text:          #1A1916   (primary text)
--muted:         #8C8880   (secondary text, labels)
--accent:        #2A5C45   (primary green — "understood / strong")
--accent-light:  #EAF2EE   (green tint backgrounds)
--shallow:       #B8860B   (amber — "shallow understanding")
--shallow-light: #FDF8E8
--missing:       #7C3D9E   (purple — "missing / not covered")
--missing-light: #F5EEFA
--warn:          #C4541A   (burnt orange — "overconfident / misconception")
--warn-light:    #FDF0E8
--dark:          #1A1916   (dark surfaces)
```

**Typography**
- Headlines / display text: `Instrument Serif` (Google Fonts)
- Body / UI text: `DM Sans` (Google Fonts)
- Never use Inter, Roboto, or system fonts

**Spacing & Shape**
- Border radius: 8px for inputs, 10–14px for cards, 7px for buttons
- Card padding: 20–28px
- Page content max-width: 1100px
- Content padding: 36px 40px

**Component Rules**
- No multiple choice anywhere in the product
- No timers visible to users
- No scores displayed as numbers (percentages or grades)
- Feedback language is always diagnostic ("you haven't connected X to Y yet") never evaluative ("wrong" or "incorrect")
- Buttons: primary = accent green, ghost = border + muted text

---

## Application Structure

```
/                          → Dashboard (home)
/analyze                   → New session: content input
/session/:id               → Active session: questions
/session/:id/feedback      → Feedback report
/sessions                  → Session history
/knowledge-map             → Visual knowledge graph
/methods                   → Learning method selector
/settings                  → Account + preferences
/login                     → Auth
/signup                    → Auth
```

---

## Screen 1: Dashboard (/)

This is the first thing a logged-in user sees. It has two dominant zones.

### Zone 1: Quick Start (top, full width)

A card with a warm off-white background, subtle green radial gradient in the top-right corner.

**Label** (small, uppercase, accent green): `ANALYZE SOMETHING NEW`

**Headline** (Instrument Serif, 22px): `What did you just learn?`

**Subtext** (muted, 13.5px): `Paste a link, upload a PDF, or drop in your notes. Serify will tell you what you actually understood.`

**Input type tabs** (row of small pill buttons, default active = YouTube URL):
- `YouTube URL`
- `Article URL`
- `PDF Upload`
- `Paste Notes`

**Input field** (changes based on active tab):
- YouTube URL: text input with placeholder `https://youtube.com/watch?v=...`
- Article URL: text input with placeholder `https://...`
- PDF Upload: file drop zone, accepts .pdf only, shows filename after selection
- Paste Notes: textarea, ~5 rows, placeholder `Paste your notes, highlights, or any text here...`

**Analyze button** (accent green, full width of input row): `Analyze →`

On click: validate input → show loading state on the card → redirect to `/session/:id` once processing is complete.

**Loading state on the card** (replaces the form while processing):
- Spinner or subtle pulse animation
- Rotating status messages every 2 seconds:
  - "Extracting content..."
  - "Identifying key concepts..."
  - "Building concept map..."
  - "Generating your questions..."
- This should feel fast and intelligent, not like a loading bar

---

### Zone 2: Knowledge Gap Summary

**Section header:** `Your Knowledge Gaps` with a link `View all →` that goes to `/knowledge-map`

**Gap cards grid** (3 columns):

Each card represents a concept from a recent session that was assessed as Shallow, Missing, or Overconfident. Cards show:
- Concept name (e.g. "Transformer Attention Mechanism")
- Source content title (small, muted)
- Mastery state tag: one of `Strongly Retained` (green), `Shallow Grasp` (amber), `Missing` (purple), `Overconfident` (burnt orange)
- A thin horizontal bar showing depth (colored by state)
- A score indicator using Instrument Serif font (not a number grade — a qualitative symbol or a subtle visual)
- On hover: slight lift (translateY -2px) + soft shadow
- On click: goes to `/session/:id/feedback` for that session

**Empty state** (first-time user): A single card with dashed border that says "Complete your first session to see your gaps here."

---

### Zone 3: Insight Strip (below gap cards)

A dark (#1A1916) rounded strip that surfaces one AI-generated insight based on the user's session history. Example: "You consistently understand mechanisms but miss boundary conditions — seen across 4 sessions." With a `Review Now →` button on the right that links to the relevant session. If no history exists, hide this strip.

---

### Zone 4: Recent Sessions (below insight strip)

**Section header:** `Recent Sessions`

A list of session rows. Each row shows:
- An icon indicating content type (YouTube = red tint, PDF = orange tint, Article = green tint)
- Content title
- Date + time
- Content type badge
- A mini bar visualization showing the split between Retained / Shallow / Missing concepts
- If the session had a significant overconfidence gap: show a small warning tag `⚠ Confidence gap detected`
- On click: goes to `/session/:id/feedback`

Show maximum 5 recent sessions. Below the list: `View all sessions →` linking to `/sessions`.

---

## Screen 2: New Session — Content Input (/analyze)

This screen is also reachable directly (not just from the dashboard Quick Start). Same input form as the Quick Start card but full-page, centered layout with more breathing room. Use this when users navigate directly to `/analyze`.

After hitting Analyze, show the same loading state described above. Then redirect to `/session/:id`.

---

## Screen 3: Active Session (/session/:id)

This is where the user answers questions. The UI must feel calm, focused, and unhurried — like a blank journal page.

### Layout

Left sidebar (fixed, 260px): shows the concept map extracted from the content — a simple list of concepts grouped by category. Each concept has a small dot indicator that will fill in as questions about it are answered. This gives the user a sense of what the content covered and their progress through the session.

Main area (full remaining width): one question at a time.

### Question Display

**Question type label** (small, uppercase, colored):
- `RETRIEVAL` in accent green
- `APPLICATION` in amber
- `MISCONCEPTION PROBE` in purple

**Question text** (Instrument Serif, 20px, generous line height)

**Answer textarea** (large, minimal border, soft focus state): `Write your answer here — use your own words.` as placeholder. Auto-expands as the user types. Minimum 4 rows.

**Navigation**:
- `Submit Answer →` button (accent green) — submits current answer and moves to next question
- Small text below: `Question 3 of 7` — no progress bar (too test-like), just a quiet counter

**Session controls** (top right, small):
- `Pause` — saves progress, returns to dashboard
- `Abandon session` — with confirmation dialog

### Behavior Rules
- Never show all questions at once
- Never allow going back to edit a previous answer (this preserves retrieval integrity)
- After the last answer is submitted: show a brief "Analyzing your responses..." loading screen (same rotating messages as before, adapted: "Reading your answers...", "Mapping your understanding...", "Identifying gaps...", "Building your feedback report...") then redirect to `/session/:id/feedback`

---

## Screen 4: Feedback Report (/session/:id/feedback)

This is the most important screen in the product. It must be excellent.

### Header

Content title + source type icon. Date of session. A single sentence summary generated by Claude: e.g. "You have a strong grasp of the core mechanism but several important boundary conditions are missing."

A row of four summary counts (not scores, just counts):
- `4 Strongly Retained`
- `3 Shallow`
- `2 Missing`
- `1 Misconception`

Each with its corresponding color dot.

---

### Section 1: Strength Map

Full-width section. Title: `Strength Map` (Instrument Serif).

A list of every key concept from the session. For each concept:
- Concept name
- Mastery state tag (Strongly Retained / Partial / Shallow / Missing)
- A thin depth bar
- 1–2 sentences of specific feedback explaining what was strong or what was missing in the user's answer about this concept. This is not generic. It references what the user actually wrote.

This section makes it clear that Serify read and understood both the source material and the user's answers.

---

### Section 2: Cognitive Analysis

Two-column layout.

**Left column — "What you understand well"**: A brief narrative paragraph (2–4 sentences) describing the patterns of understanding in the user's answers. What concepts did they connect well? What reasoning was solid?

**Right column — "Where understanding breaks down"**: A brief narrative paragraph describing where the user demonstrated surface familiarity without depth, where connections were missing, and where the reasoning contained errors. Specific, not generic.

Below both columns: a highlighted box for **Overconfidence Gaps** (only shown if detected). This uses the warn color (burnt orange). Text example: "You answered the question about X with high confidence and at length, but the explanation contained a fundamental error: [specific error]. This suggests a solidified misconception rather than a gap — it's important to address this specifically."

---

### Section 3: Misconception Report

Only shown if misconceptions were detected. Uses the purple missing color scheme.

Title: `Misconceptions Detected`

For each misconception:
- The concept it relates to
- What the user's answer implied they believe
- What the correct understanding actually is
- Why this matters (consequence of holding the wrong model)

This section must be written carefully — it should never feel like an accusation. Framing: "Your answer suggests a common misconception about X. Here's the distinction that matters..."

---

### Section 4: Focus Suggestions

Title: `What to Do Next` (Instrument Serif)

A list of 3–5 specific, actionable suggestions generated by Claude. Each suggestion:
- Names the specific concept to revisit
- Suggests a specific action (re-read section X, try explaining Y using the Feynman technique, find one real-world example of Z)
- Explains why this action addresses the specific gap identified

Below suggestions: two buttons:
- `Start a New Session →` (accent green)
- `Review This Content Again` (ghost button) — re-runs the session with the same source content

---

### Section 5: Share

A subtle row at the bottom: `Share your results` with a button that generates a Knowledge Report Card.

**Knowledge Report Card**: a clean, self-contained visual (shareable as an image) showing:
- "I just analyzed: [Content Title]"
- The four mastery state counts
- The single biggest insight from the session
- Serify branding

This is designed to be posted on social media. It should look good and feel like an honest reflection, not a brag.

---

## Screen 5: Session History (/sessions)

A full list of all past sessions. Filterable by:
- Content type (YouTube, Article, PDF, Notes)
- Mastery outcome (sessions with strong results vs. sessions with significant gaps)
- Date range

Each session row same design as on the dashboard. Clicking goes to `/session/:id/feedback`.

A search bar at the top to search by content title or concept name.

---

## Screen 6: Knowledge Map (/knowledge-map)

A visual, interactive node graph built with D3.js or a similar library.

Every concept ever extracted from a user's sessions is a node. Edges connect concepts that appeared in the same session or that Claude identified as related. Node size = number of sessions that touched this concept. Node color = current mastery level (green / amber / purple / burnt orange).

**Interactions:**
- Hover a node: shows concept name, current mastery state, number of sessions
- Click a node: opens a side panel showing all sessions that covered this concept, the user's mastery trend over time (simple line or dot sequence showing Shallow → Partial → Strong), and a button to start a new session on this concept
- Zoom and pan
- Filter by mastery state (show only gaps, show only strong concepts, etc.)

**Empty state**: A friendly illustration and text: "Your knowledge map will grow with every session. Start your first analysis to see it come to life."

---

## Screen 7: Methods (/methods)

A clean grid of learning method cards. Each card:
- Method name (Instrument Serif)
- 2-sentence description of what it does and who it's for
- A "Use this method" button

**Available methods:**

`Feynman Mode` — Questions ask the user to explain concepts as if teaching someone with no background. Feedback specifically evaluates clarity, completeness, and where the explanation breaks down.

`Spaced Repetition Mode` — Instead of new content, this mode surfaces concepts from past sessions that are due for review based on the forgetting curve. No new content input needed.

`Socratic Mode` — Runs as a dialogue. One question at a time, and Serify follows up based on the user's answer — probing deeper, challenging, asking for clarification. Most intensive mode.

`Practice Quiz Mode` — Higher volume, shorter questions, faster pace. More breadth than depth. Good for exam prep. Still free-text, never multiple choice.

`Standard Mode` — The default. Balanced mix of retrieval, application, and misconception questions.

Selected method persists as a user preference but can be changed per session. When starting a new session from `/analyze`, a small "Method" selector shows the current method with an option to change it.

---

## Screen 8: Settings (/settings)

Standard settings page. Sections:

**Account**: Name, email, profile photo, password change, connected accounts (Google).

**Learning Preferences**: Default learning method. Notification preferences (weekly digest email on/off). Language preference for feedback (more technical vs. more accessible).

**Data**: Export all session data as JSON. Delete account.

**Subscription**: Current plan, upgrade CTA if on free tier, billing management link.

---

## Core AI Flows

These are the Claude API calls that power the product. Each must be carefully prompted.

### Flow 1: Content Processing

**Input:** Raw text (transcript, article text, PDF text, or notes)

**Prompt task:** Extract a structured concept map from the content. For each concept, identify: concept name, a 1-sentence definition, its relationship to other concepts in the material, its importance to the overall content (primary / secondary / contextual), and a "misconception risk" flag if this concept is commonly misunderstood.

**Output format:** JSON — array of concept objects.

**Used for:** Building the session concept map shown in the sidebar, generating questions, evaluating answers.

---

### Flow 2: Question Generation

**Input:** Concept map JSON + selected learning method

**Prompt task:** Generate N questions (scaled to content complexity) across the three types: Retrieval, Application, Misconception Probe. Each question must target a specific concept from the map and be designed to reveal the difference between surface familiarity and genuine understanding. Questions must be open-ended and require free-text answers. No yes/no questions. No multiple choice.

**Output format:** JSON — array of question objects, each with: question text, type (retrieval/application/misconception), target concept ID, what a strong answer would demonstrate, what a weak answer typically looks like.

---

### Flow 3: Answer Analysis

**Input:** Question object + user's answer + source concept map

**Prompt task:** Analyze the user's answer against the concept map. Assess: factual accuracy, conceptual depth (surface vs. mechanistic vs. relational understanding), misconception detection, and confidence calibration (inferred from answer length and certainty of language). Return a structured assessment for this answer.

**Output format:** JSON — mastery state for the targeted concept, specific feedback text, misconception flag + description if detected, confidence level inferred.

**Critical:** This must be called per answer, not all at once, so feedback can be shown in real-time if needed and partial sessions can be recovered.

---

### Flow 4: Feedback Report Generation

**Input:** All answer assessments from the session + source concept map

**Prompt task:** Synthesize all answer assessments into a coherent feedback report. Generate: the header summary sentence, the Strength Map entries for each concept with specific feedback text, the Cognitive Analysis narrative paragraphs (left and right columns), the Misconception Report entries if applicable, and the Focus Suggestions list.

**Tone requirement:** Diagnostic and curious, never evaluative. First-person from Serify's perspective: "Your answers show..." not "You failed to..." Language should feel like a thoughtful mentor, not a grading system.

**Output format:** Structured JSON matching the feedback report sections.

---

### Flow 5: Weekly Insight Generation

**Input:** All sessions from the past 7 days for this user

**Prompt task:** Identify the top 3 recurring knowledge gaps — concepts or concept types that have appeared as Shallow or Missing across multiple sessions. Generate a 1-sentence insight for each. Also generate the single most important insight for the dashboard insight strip.

**Output format:** JSON — array of gap objects + one insight string.

**Triggered:** Weekly cron job per user.

---

## Data Models

```
User
  id, email, name, plan (free/pro/team), created_at, preferences (JSON)

Session
  id, user_id, content_type (youtube/article/pdf/notes), content_url,
  content_title, raw_text, concept_map (JSON), method, status
  (processing/active/complete/abandoned), created_at, completed_at

Question
  id, session_id, type (retrieval/application/misconception),
  question_text, target_concept_id, order_index

Answer
  id, question_id, session_id, user_id, answer_text,
  submitted_at, assessment (JSON)

Concept
  id, session_id, name, definition, importance, misconception_risk,
  relationships (JSON)

FeedbackReport
  id, session_id, summary_sentence, strength_map (JSON),
  cognitive_analysis (JSON), misconception_report (JSON),
  focus_suggestions (JSON), generated_at

KnowledgeNode (for the persistent knowledge map)
  id, user_id, concept_name, canonical_name, session_ids (array),
  mastery_history (JSON array of {date, state}), current_mastery
```

---

## State Management

The session flow has several states that must be handled correctly:

- `processing` — content is being ingested and questions generated. User sees loading screen.
- `active` — user is answering questions. Current question index tracked.
- `analyzing` — user submitted last answer, feedback is being generated.
- `complete` — feedback report is ready.
- `abandoned` — user exited mid-session. Session is recoverable from the dashboard.

If a user closes the browser mid-session, their answers so far must be saved. When they return, offer to resume or start over.

---

## Free vs. Pro Gating

**Free tier:**
- 3 sessions per month
- Standard mode only
- 7-day session history
- Basic strength map in feedback
- No Knowledge Graph
- No sharing features

**Pro tier ($12/month):**
- Unlimited sessions
- All learning method modes
- Full session history
- Complete feedback report including Cognitive Analysis, Misconception Report, Focus Suggestions
- Knowledge Graph
- Share / Knowledge Report Card
- Weekly digest email
- PDF batch upload (up to 5 at once)

**Gate behavior:** When a free user hits a Pro feature, show a non-intrusive inline upgrade prompt — a small card within the feature area, not a modal takeover. Copy: "This is a Pro feature. Upgrade to unlock your full feedback report." with an `Upgrade →` button.

---

## Sidebar (Persistent Navigation)

Fixed left sidebar, 220px wide, visible on all authenticated screens.

**Top:** Serify logo in Instrument Serif + tagline "reflection engine" in muted small text.

**Nav items** (with icons):
- Home (dashboard)
- New Session
- Sessions
- Knowledge Map
- Methods
- Settings

Active state: soft green tint background, accent green text.

**Bottom:** User avatar (initials in a green circle), name, plan badge. Clicking opens a mini dropdown with Settings and Sign Out.

---

## Empty States

Every screen needs a thoughtful empty state for new users.

- Dashboard gap cards: dashed-border card, "Complete your first session to see your gaps."
- Session history: centered illustration + "No sessions yet. Analyze something to get started." + `Analyze Now →` button
- Knowledge Map: "Your knowledge map grows with every session." + illustration + `Start Your First Session →`
- Insight strip: hidden entirely until 2+ sessions exist

---

## Error States

- Content ingestion fails (bad URL, paywalled article, no transcript available): show inline error on the input card with a specific message. "We couldn't access this YouTube video — it may be private or have transcripts disabled." Never a generic error.
- AI generation fails: "Something went wrong generating your questions. Your content was saved — try again." with a retry button.
- Session answer save fails: silent retry 3x, then show a small banner "Having trouble saving — check your connection."

---

## Responsive Behavior

The app is primarily a desktop/tablet tool. Mobile support is required but mobile is a secondary use case.

- Below 768px: sidebar collapses to a bottom tab bar
- Below 768px: gap cards stack to single column
- Below 768px: feedback report sections stack vertically (no two-column layout)
- The question answer textarea must be comfortable to type in on mobile

---

## Demo Mode

For the agent to implement a working demo mode:

When accessed at `/?demo=true` or via a "Try Demo" button on the login page, load a pre-seeded demo session with:
- A fake YouTube video about "How Transformer Models Work"
- A pre-built concept map with 8 concepts
- Pre-generated questions (one of each type: retrieval, application, misconception probe)
- Allow the user to type real answers
- Run real AI analysis on those answers and show real feedback
- Do not require login for demo mode
- Show a subtle "You're in demo mode — sign up to save your results" banner at the top

The demo must demonstrate the full loop: input → questions → feedback report. It is the primary conversion tool.

---

## Launch Checklist

Before shipping, verify:

- [ ] Full session loop works end to end (input → questions → answers → feedback)
- [ ] All four content input types work (YouTube, Article, PDF, Notes)
- [ ] All five AI flows return structured JSON and handle errors gracefully
- [ ] Feedback report shows concept-specific, personalized feedback (not generic)
- [ ] Knowledge Graph renders and is interactive
- [ ] All five learning method modes generate appropriately different questions
- [ ] Free tier gating works correctly
- [ ] Demo mode works without login
- [ ] Session recovery works (resume after browser close)
- [ ] Weekly digest email sends correctly
- [ ] Mobile layout is functional
- [ ] Empty states and error states are implemented on every screen
- [ ] Design system is consistent across all screens (colors, fonts, spacing)
- [ ] No multiple choice questions appear anywhere in the product
- [ ] No number scores appear anywhere in the feedback report
- [ ] Feedback language is diagnostic not evaluative throughout